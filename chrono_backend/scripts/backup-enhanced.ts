#!/usr/bin/env tsx
/**
 * Script de Backup Am√©lior√© pour Chrono Backend
 * 
 * Fonctionnalit√©s:
 * - Backup PostgreSQL (pg_dump)
 * - Backup Supabase (via API)
 * - Compression automatique
 * - Rotation des backups (garder N derniers)
 * - V√©rification d'int√©grit√©
 * - Logging d√©taill√©
 * 
 * Usage:
 *   npm run backup:postgres
 *   npm run backup:supabase
 *   npm run backup:all
 *   npm run backup:test
 */

import { exec } from 'child_process';
import { promisify } from 'util';
import fs from 'fs/promises';
import path from 'path';
import { createClient } from '@supabase/supabase-js';
import { fileURLToPath } from 'url';

const execAsync = promisify(exec);
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Configuration
const BACKUP_DIR = process.env.BACKUP_DIR || path.join(__dirname, '../backups');
const MAX_BACKUPS = parseInt(process.env.MAX_BACKUPS || '30'); // Garder 30 jours de backups
const COMPRESS = process.env.COMPRESS_BACKUPS !== 'false'; // Compression par d√©faut

interface BackupResult {
  success: boolean;
  type: 'postgres' | 'supabase' | 'all';
  filePath?: string;
  size?: number;
  error?: string;
  timestamp: string;
}

/**
 * Cr√©er le r√©pertoire de backup s'il n'existe pas
 */
async function ensureBackupDir(): Promise<void> {
  try {
    await fs.mkdir(BACKUP_DIR, { recursive: true });
  } catch (error: any) {
    if (error.code !== 'EEXIST') {
      throw error;
    }
  }
}

/**
 * Formater la date pour les noms de fichiers
 */
function formatDate(date: Date = new Date()): string {
  return date.toISOString().replace(/[:.]/g, '-').split('T')[0] + '_' + 
         date.toTimeString().split(' ')[0].replace(/:/g, '-');
}

/**
 * Obtenir la taille d'un fichier en MB
 */
async function getFileSize(filePath: string): Promise<number> {
  try {
    const stats = await fs.stat(filePath);
    return Math.round((stats.size / (1024 * 1024)) * 100) / 100; // MB
  } catch {
    return 0;
  }
}

/**
 * Compresser un fichier avec gzip
 */
async function compressFile(filePath: string): Promise<string> {
  if (!COMPRESS) return filePath;
  
  const compressedPath = `${filePath}.gz`;
  try {
    await execAsync(`gzip -f "${filePath}"`);
    return compressedPath;
  } catch (error: any) {
    console.error(`Erreur lors de la compression: ${error.message}`);
    return filePath;
  }
}

/**
 * Backup PostgreSQL
 */
async function backupPostgres(): Promise<BackupResult> {
  const timestamp = formatDate();
  const fileName = `postgres_backup_${timestamp}.sql`;
  const filePath = path.join(BACKUP_DIR, fileName);

  try {
    const databaseUrl = process.env.DATABASE_URL;
    if (!databaseUrl) {
      throw new Error('DATABASE_URL non configur√©');
    }

    console.log('üîÑ D√©marrage du backup PostgreSQL...');
    
    // Extraire les informations de connexion depuis DATABASE_URL
    // Format: postgresql://user:password@host:port/database
    const url = new URL(databaseUrl);
    const host = url.hostname;
    const port = url.port || '5432';
    const database = url.pathname.slice(1); // Enlever le premier /
    const user = url.username;
    const password = url.password;

    // Exporter le mot de passe pour pg_dump
    process.env.PGPASSWORD = password;

    // Commande pg_dump
    const dumpCommand = `pg_dump -h ${host} -p ${port} -U ${user} -d ${database} -F c -f "${filePath}"`;

    await execAsync(dumpCommand);

    // V√©rifier que le fichier existe et n'est pas vide
    const stats = await fs.stat(filePath);
    if (stats.size === 0) {
      throw new Error('Le fichier de backup est vide');
    }

    console.log(`‚úÖ Backup PostgreSQL cr√©√©: ${filePath} (${stats.size} bytes)`);

    // Compresser si activ√©
    let finalPath = filePath;
    if (COMPRESS) {
      finalPath = await compressFile(filePath);
      const compressedSize = await getFileSize(finalPath);
      console.log(`üì¶ Backup compress√©: ${compressedSize} MB`);
    }

    const size = await getFileSize(finalPath);

    return {
      success: true,
      type: 'postgres',
      filePath: finalPath,
      size,
      timestamp: new Date().toISOString(),
    };
  } catch (error: any) {
    console.error(`‚ùå Erreur lors du backup PostgreSQL: ${error.message}`);
    return {
      success: false,
      type: 'postgres',
      error: error.message,
      timestamp: new Date().toISOString(),
    };
  } finally {
    // Nettoyer le mot de passe de l'environnement
    delete process.env.PGPASSWORD;
  }
}

/**
 * Backup Supabase (via pg_dump de la connection string Supabase)
 */
async function backupSupabase(): Promise<BackupResult> {
  const timestamp = formatDate();
  const fileName = `supabase_backup_${timestamp}.sql`;
  const filePath = path.join(BACKUP_DIR, fileName);

  try {
    const supabaseUrl = process.env.SUPABASE_URL;
    const supabaseDbUrl = process.env.SUPABASE_DB_URL || process.env.DATABASE_URL;

    if (!supabaseDbUrl) {
      throw new Error('SUPABASE_DB_URL ou DATABASE_URL non configur√©');
    }

    console.log('üîÑ D√©marrage du backup Supabase...');

    // Utiliser la m√™me m√©thode que PostgreSQL
    const url = new URL(supabaseDbUrl);
    const host = url.hostname;
    const port = url.port || '5432';
    const database = url.pathname.slice(1);
    const user = url.username;
    const password = url.password;

    process.env.PGPASSWORD = password;

    const dumpCommand = `pg_dump -h ${host} -p ${port} -U ${user} -d ${database} -F c -f "${filePath}"`;

    await execAsync(dumpCommand);

    const stats = await fs.stat(filePath);
    if (stats.size === 0) {
      throw new Error('Le fichier de backup est vide');
    }

    console.log(`‚úÖ Backup Supabase cr√©√©: ${filePath} (${stats.size} bytes)`);

    let finalPath = filePath;
    if (COMPRESS) {
      finalPath = await compressFile(filePath);
      const compressedSize = await getFileSize(finalPath);
      console.log(`üì¶ Backup compress√©: ${compressedSize} MB`);
    }

    const size = await getFileSize(finalPath);

    return {
      success: true,
      type: 'supabase',
      filePath: finalPath,
      size,
      timestamp: new Date().toISOString(),
    };
  } catch (error: any) {
    console.error(`‚ùå Erreur lors du backup Supabase: ${error.message}`);
    return {
      success: false,
      type: 'supabase',
      error: error.message,
      timestamp: new Date().toISOString(),
    };
  } finally {
    delete process.env.PGPASSWORD;
  }
}

/**
 * Nettoyer les anciens backups (garder seulement les N derniers)
 */
async function cleanupOldBackups(type: 'postgres' | 'supabase' | 'all'): Promise<void> {
  try {
    const files = await fs.readdir(BACKUP_DIR);
    
    // Filtrer les fichiers de backup
    const backupFiles = files
      .filter(file => {
        if (type === 'postgres') return file.startsWith('postgres_backup_');
        if (type === 'supabase') return file.startsWith('supabase_backup_');
        return file.startsWith('postgres_backup_') || file.startsWith('supabase_backup_');
      })
      .map(file => ({
        name: file,
        path: path.join(BACKUP_DIR, file),
      }));

    // Trier par date (plus r√©cent en premier)
    const filesWithStats = await Promise.all(
      backupFiles.map(async (file) => {
        const stats = await fs.stat(file.path);
        return {
          ...file,
          mtime: stats.mtime,
        };
      })
    );

    filesWithStats.sort((a, b) => b.mtime.getTime() - a.mtime.getTime());

    // Supprimer les fichiers au-del√† de MAX_BACKUPS
    const filesToDelete = filesWithStats.slice(MAX_BACKUPS);
    
    if (filesToDelete.length > 0) {
      console.log(`üßπ Suppression de ${filesToDelete.length} ancien(s) backup(s)...`);
      for (const file of filesToDelete) {
        await fs.unlink(file.path);
        console.log(`  ‚úì Supprim√©: ${file.name}`);
      }
    }
  } catch (error: any) {
    console.error(`‚ö†Ô∏è Erreur lors du nettoyage: ${error.message}`);
  }
}

/**
 * V√©rifier l'int√©grit√© d'un backup
 */
async function verifyBackup(filePath: string): Promise<boolean> {
  try {
    const stats = await fs.stat(filePath);
    
    // V√©rifier que le fichier existe et n'est pas vide
    if (stats.size === 0) {
      console.error(`‚ùå Le backup est vide: ${filePath}`);
      return false;
    }

    // Si c'est un fichier compress√©, v√©rifier qu'il peut √™tre d√©compress√©
    if (filePath.endsWith('.gz')) {
      try {
        await execAsync(`gunzip -t "${filePath}"`);
        console.log(`‚úÖ Backup compress√© valide: ${filePath}`);
      } catch {
        console.error(`‚ùå Backup compress√© corrompu: ${filePath}`);
        return false;
      }
    }

    // Pour les fichiers .sql, v√©rifier qu'ils contiennent du SQL valide
    if (filePath.endsWith('.sql')) {
      const content = await fs.readFile(filePath, 'utf-8');
      if (!content.includes('PostgreSQL database dump') && !content.includes('CREATE TABLE')) {
        console.error(`‚ùå Le backup ne semble pas √™tre un dump PostgreSQL valide`);
        return false;
      }
    }

    return true;
  } catch (error: any) {
    console.error(`‚ùå Erreur lors de la v√©rification: ${error.message}`);
    return false;
  }
}

/**
 * Sauvegarder les m√©tadonn√©es du backup
 */
async function saveBackupMetadata(result: BackupResult): Promise<void> {
  const metadataFile = path.join(BACKUP_DIR, 'backup_metadata.json');
  
  try {
    let metadata: BackupResult[] = [];
    
    // Lire les m√©tadonn√©es existantes
    try {
      const content = await fs.readFile(metadataFile, 'utf-8');
      metadata = JSON.parse(content);
    } catch {
      // Fichier n'existe pas encore, cr√©er un nouveau
    }

    // Ajouter le nouveau backup
    metadata.push(result);

    // Garder seulement les N derniers
    metadata = metadata.slice(-MAX_BACKUPS);

    // Sauvegarder
    await fs.writeFile(metadataFile, JSON.stringify(metadata, null, 2));
  } catch (error: any) {
    console.error(`‚ö†Ô∏è Erreur lors de la sauvegarde des m√©tadonn√©es: ${error.message}`);
  }
}

/**
 * Fonction principale
 */
async function main() {
  const args = process.argv.slice(2);
  const type = args[0] || 'all';

  console.log('üöÄ D√©marrage du processus de backup...');
  console.log(`üìÅ R√©pertoire de backup: ${BACKUP_DIR}`);
  console.log(`üì¶ Compression: ${COMPRESS ? 'Activ√©e' : 'D√©sactiv√©e'}`);
  console.log(`üóëÔ∏è  R√©tention: ${MAX_BACKUPS} backups maximum\n`);

  await ensureBackupDir();

  const results: BackupResult[] = [];

  try {
    if (type === 'postgres' || type === 'all') {
      const result = await backupPostgres();
      results.push(result);
      
      if (result.success && result.filePath) {
        const isValid = await verifyBackup(result.filePath);
        if (isValid) {
          await saveBackupMetadata(result);
          await cleanupOldBackups('postgres');
        } else {
          console.error('‚ùå Le backup PostgreSQL a √©chou√© la v√©rification d\'int√©grit√©');
        }
      }
    }

    if (type === 'supabase' || type === 'all') {
      const result = await backupSupabase();
      results.push(result);
      
      if (result.success && result.filePath) {
        const isValid = await verifyBackup(result.filePath);
        if (isValid) {
          await saveBackupMetadata(result);
          await cleanupOldBackups('supabase');
        } else {
          console.error('‚ùå Le backup Supabase a √©chou√© la v√©rification d\'int√©grit√©');
        }
      }
    }

    // R√©sum√©
    console.log('\nüìä R√©sum√© des backups:');
    const successful = results.filter(r => r.success);
    const failed = results.filter(r => !r.success);

    if (successful.length > 0) {
      console.log(`‚úÖ ${successful.length} backup(s) r√©ussi(s):`);
      successful.forEach(r => {
        console.log(`   - ${r.type}: ${r.filePath} (${r.size} MB)`);
      });
    }

    if (failed.length > 0) {
      console.log(`‚ùå ${failed.length} backup(s) √©chou√©(s):`);
      failed.forEach(r => {
        console.log(`   - ${r.type}: ${r.error}`);
      });
      process.exit(1);
    }

    console.log('\n‚úÖ Tous les backups ont √©t√© cr√©√©s avec succ√®s!');
  } catch (error: any) {
    console.error(`\n‚ùå Erreur fatale: ${error.message}`);
    process.exit(1);
  }
}

// Ex√©cuter si appel√© directement
if (import.meta.url === `file://${process.argv[1]}`) {
  main();
}

export { backupPostgres, backupSupabase, verifyBackup, cleanupOldBackups };

